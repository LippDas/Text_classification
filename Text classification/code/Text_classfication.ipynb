{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marka</th>\n",
       "      <th>marka_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACURA</td>\n",
       "      <td>ACURA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACURA</td>\n",
       "      <td>ACURA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACURA</td>\n",
       "      <td>ACURA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACURA</td>\n",
       "      <td>ACURA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACURA</td>\n",
       "      <td>ACURA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12534</th>\n",
       "      <td>WAZ</td>\n",
       "      <td>LADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>WAZ</td>\n",
       "      <td>LADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12536</th>\n",
       "      <td>WAZ</td>\n",
       "      <td>LADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537</th>\n",
       "      <td>WAZ</td>\n",
       "      <td>LADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12538</th>\n",
       "      <td>WSM-BIELSKO BIA?A</td>\n",
       "      <td>FIAT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12539 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   marka marka_pop\n",
       "0                  ACURA     ACURA\n",
       "1                  ACURA     ACURA\n",
       "2                  ACURA     ACURA\n",
       "3                  ACURA     ACURA\n",
       "4                  ACURA     ACURA\n",
       "...                  ...       ...\n",
       "12534                WAZ      LADA\n",
       "12535                WAZ      LADA\n",
       "12536                WAZ      LADA\n",
       "12537                WAZ      LADA\n",
       "12538  WSM-BIELSKO BIA?A      FIAT\n",
       "\n",
       "[12539 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/lippd/Desktop/ML in R/Python/Text classification/data/cars.csv', sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_size):\n",
    "    train = df[:train_size]\n",
    "    test = df[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat, test_cat = train_test_split(df['marka_pop'], train_size)\n",
    "train_text, test_text = train_test_split(df['marka'], train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, ..., 3, 3, 2])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words = max_words, char_level = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_text)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "x_test = tokenize.texts_to_matrix(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.fit_transform(train_cat)\n",
    "y_test = encoder.fit_transform(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10031, 200)\n",
      "x_test shape: (2508, 200)\n",
      "y_train shape: (10031, 54)\n",
      "y_test shape: (2508, 54)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "drop_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape = (max_words,)))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 2.3785 - accuracy: 0.9053 - val_loss: 4.1381 - val_accuracy: 0.0287\n",
      "Epoch 2/2\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9880 - val_loss: 4.4762 - val_accuracy: 0.0285\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size = batch_size,\n",
    "                    epochs = 2,\n",
    "                    verbose = 1,\n",
    "                    validation_split = 0.8\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(batch_size, epochs, drop_ratio):\n",
    "  print('batch size: {}, epochs: {}, drop_ratio: {}'.format(\n",
    "      batch_size, epochs, drop_ratio))\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(512, input_shape=(max_words,)))\n",
    "  model.add(layers.Activation('relu'))\n",
    "  model.add(layers.Dropout(drop_ratio))\n",
    "  model.add(layers.Dense(num_classes))\n",
    "  model.add(layers.Activation('softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1)\n",
    "  score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=0)\n",
    "  print('\\tTest loss:', score[0])\n",
    "  print('\\tTest accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 50, epochs: 5, drop_ratio: 0.5\n",
      "\tTest loss: 3.8560774326324463\n",
      "\tTest accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 5\n",
    "drop_ratio = 0.5\n",
    "run_experiment(batch_size, epochs, drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SATURN ...\n",
      "Actual label:OPEL\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n",
      "SEAT ...\n",
      "Actual label:SEAT\n",
      "Predicted label: SMART\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_labels = encoder.classes_ \n",
    "\n",
    "for i in range(15):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_text.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_cat.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02759067, 0.05412173, 0.0119994 , 0.0263502 , 0.18642126,\n",
       "        0.03889773, 0.20183057, 0.00941428, 0.00982748, 0.0088524 ,\n",
       "        0.00932147, 0.00955747, 0.00867669, 0.00911953, 0.00978873,\n",
       "        0.00861238, 0.00990461, 0.00979252, 0.01019414, 0.00897636,\n",
       "        0.00940869, 0.01000494, 0.00905277, 0.00874915, 0.00938889,\n",
       "        0.00820053, 0.00953651, 0.00956135, 0.01027953, 0.00983542,\n",
       "        0.00960448, 0.00867443, 0.00947839, 0.01039154, 0.01036473,\n",
       "        0.01086401, 0.01004962, 0.00981695, 0.01030339, 0.01002719,\n",
       "        0.00864511, 0.00995019, 0.01025532, 0.01032149, 0.00881856,\n",
       "        0.00923786, 0.00852811, 0.00942334, 0.00971805, 0.01419165,\n",
       "        0.00943137, 0.01005135, 0.00881563, 0.00976979]], dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
